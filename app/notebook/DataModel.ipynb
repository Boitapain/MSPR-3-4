{
	"cells": [
		{
			"cell_type": "code",
			"execution_count": 1,
			"id": "1f8a0879",
			"metadata": {},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"⚠️ Incohérence pour 'Algeria' au mois de 2020-03-01 00:00:00 [Recovered]: 46 ≠ 0 + 200 (tolérance ±100)\n",
						"⚠️ Incohérence pour 'Benin' au mois de 2020-05-01 00:00:00 [Confirmed]: 232 ≠ 64 + 377 (tolérance ±100)\n",
						"⚠️ Incohérence pour 'China' au mois de 2020-04-01 00:00:00 [Recovered]: 76951 ≠ 74645 + 4006 (tolérance ±100)\n",
						"⚠️ Incohérence pour 'Cote d'Ivoire' au mois de 2020-06-01 00:00:00 [Recovered]: 4273 ≠ 1435 + 3460 (tolérance ±100)\n",
						"⚠️ Incohérence pour 'Ecuador' au mois de 2020-05-01 00:00:00 [Confirmed]: 39098 ≠ 24934 + 17277 (tolérance ±100)\n",
						"⚠️ Incohérence pour 'Finland' au mois de 2020-05-01 00:00:00 [Recovered]: 5500 ≠ 3000 + 2900 (tolérance ±100)\n",
						"⚠️ Incohérence pour 'France' au mois de 2020-04-01 00:00:00 [Confirmed]: 167299 ≠ 52827 + 119170 (tolérance ±100)\n",
						"⚠️ Incohérence pour 'France' au mois de 2020-05-01 00:00:00 [Deaths]: 28805 ≠ 24379 + 4860 (tolérance ±100)\n",
						"⚠️ Incohérence pour 'France' au mois de 2020-05-01 00:00:00 [Confirmed]: 189009 ≠ 167299 + 22095 (tolérance ±100)\n",
						"⚠️ Incohérence pour 'France' au mois de 2020-06-01 00:00:00 [Confirmed]: 202063 ≠ 189009 + 14539 (tolérance ±100)\n",
						"⚠️ Incohérence pour 'Italy' au mois de 2020-06-01 00:00:00 [Confirmed]: 240578 ≠ 232997 + 7729 (tolérance ±100)\n",
						"⚠️ Incohérence pour 'Jordan' au mois de 2020-05-01 00:00:00 [Recovered]: 522 ≠ 362 + 338 (tolérance ±100)\n",
						"⚠️ Incohérence pour 'Jordan' au mois de 2020-07-01 00:00:00 [Confirmed]: 1176 ≠ 1132 + 154 (tolérance ±100)\n",
						"⚠️ Incohérence pour 'Kazakhstan' au mois de 2020-07-01 00:00:00 [Recovered]: 54404 ≠ 13558 + 46120 (tolérance ±100)\n",
						"⚠️ Incohérence pour 'Lithuania' au mois de 2020-04-01 00:00:00 [Confirmed]: 1385 ≠ 537 + 953 (tolérance ±100)\n",
						"⚠️ Incohérence pour 'Mexico' au mois de 2020-07-01 00:00:00 [Recovered]: 303810 ≠ 174538 + 161868 (tolérance ±100)\n",
						"⚠️ Incohérence pour 'Peru' au mois de 2020-06-01 00:00:00 [Recovered]: 174535 ≠ 67208 + 110155 (tolérance ±100)\n",
						"⚠️ Incohérence pour 'Portugal' au mois de 2020-05-01 00:00:00 [Confirmed]: 32500 ≠ 25045 + 7616 (tolérance ±100)\n",
						"⚠️ Incohérence pour 'Serbia' au mois de 2020-06-01 00:00:00 [Recovered]: 12662 ≠ 6698 + 6282 (tolérance ±100)\n",
						"⚠️ Incohérence pour 'Serbia' au mois de 2020-07-01 00:00:00 [Recovered]: 0 ≠ 12662 + 18466 (tolérance ±100)\n",
						"⚠️ Incohérence pour 'Spain' au mois de 2020-04-01 00:00:00 [Confirmed]: 213435 ≠ 95923 + 127546 (tolérance ±100)\n",
						"⚠️ Incohérence pour 'Spain' au mois de 2020-05-01 00:00:00 [Deaths]: 27127 ≠ 24543 + 6420 (tolérance ±100)\n",
						"⚠️ Incohérence pour 'Spain' au mois de 2020-05-01 00:00:00 [Confirmed]: 239479 ≠ 213435 + 26416 (tolérance ±100)\n",
						"⚠️ Incohérence pour 'Tajikistan' au mois de 2020-05-01 00:00:00 [Recovered]: 2004 ≠ 0 + 2346 (tolérance ±100)\n",
						"⚠️ Incohérence pour 'US' au mois de 2020-05-01 00:00:00 [Recovered]: 444758 ≠ 153947 + 295703 (tolérance ±100)\n",
						"⚠️ Incohérence pour 'Uganda' au mois de 2020-05-01 00:00:00 [Confirmed]: 417 ≠ 83 + 438 (tolérance ±100)\n",
						"⚠️ Incohérence pour 'Uganda' au mois de 2020-07-01 00:00:00 [Recovered]: 986 ≠ 819 + 393 (tolérance ±100)\n",
						"⚠️ Incohérence pour 'United Kingdom' au mois de 2020-04-01 00:00:00 [Recovered]: 859 ≠ 179 + 1324 (tolérance ±100)\n",
						"\n",
						"ETL terminé. Fichier sauvegardé sous : data_etl_output.csv\n"
					]
				}
			],
			"source": [
				"import pandas as pd\n",
				"\n",
				"# Charger les données\n",
				"df = pd.read_csv(\"full_grouped.csv\")  \n",
				"\n",
				"# Renommer les colonnes pour simplifier\n",
				"df = df.rename(columns={\n",
				"    'Date': 'Date',\n",
				"    'Country/Region': 'Country',\n",
				"    'Confirmed': 'Confirmed',\n",
				"    'Deaths': 'Deaths',\n",
				"    'Recovered': 'Recovered',\n",
				"    'New cases': 'New cases',\n",
				"    'New deaths': 'New deaths',\n",
				"    'New recovered': 'New recovered'\n",
				"})\n",
				"\n",
				"# Convertir la date et créer une colonne mois\n",
				"df['Date'] = pd.to_datetime(df['Date'])\n",
				"df['Month'] = df['Date'].dt.to_period('M').dt.to_timestamp()\n",
				"\n",
				"# Garder uniquement les colonnes nécessaires\n",
				"df = df[['Month', 'Country', 'Confirmed', 'Deaths', 'Recovered',\n",
				"         'New cases', 'New deaths', 'New recovered']]\n",
				"\n",
				"# Convertir les colonnes temporelles en valeurs absolues\n",
				"for col in ['New cases', 'New deaths', 'New recovered']:\n",
				"    df[col] = df[col].abs()\n",
				"\n",
				"# Grouper par mois et pays\n",
				"df_grouped = df.groupby(['Month', 'Country'], as_index=False).agg({\n",
				"    'Confirmed': 'last',\n",
				"    'Deaths': 'last',\n",
				"    'Recovered': 'last',\n",
				"    'New cases': 'sum',\n",
				"    'New deaths': 'sum',\n",
				"    'New recovered': 'sum'\n",
				"})\n",
				"\n",
				"# Ajouter une colonne Id auto-incrémentée\n",
				"df_grouped.insert(0, 'Id', range(1, len(df_grouped) + 1))\n",
				"\n",
				"# Ajouter une colonne 'nom' avec la valeur 'COVID-19' au début du DataFrame\n",
				"df_grouped.insert(1, 'Name', 'COVID-19')\n",
				"\n",
				"# Vérifier la cohérence des données\n",
				"if df.isnull().any().any():\n",
				"    print(\"Les données contiennent des valeurs nulles. Vérifiez le fichier source.\")\n",
				"\n",
				"# Vérifier que les colonnes numériques ne contiennent pas de valeurs négatives\n",
				"for col in ['Confirmed', 'Deaths', 'Recovered', 'New cases', 'New deaths', 'New recovered']:\n",
				"    if (df[col] < 0).any():\n",
				"        print(f\"La colonne '{col}' contient des valeurs négatives.\")\n",
				"\n",
				"# Vérifier que les colonnes nécessaires sont présentes\n",
				"required_columns = ['Month', 'Country', 'Confirmed', 'Deaths', 'Recovered', 'New cases', 'New deaths', 'New recovered']\n",
				"for col in required_columns:\n",
				"    if col not in df.columns:\n",
				"        print(f\"La colonne requise '{col}' est manquante dans les données.\")\n",
				"\n",
				"MARGIN = 100  # Tolérance pour les incohérences\n",
				"# Vérification des incohérences\n",
				"# Initialiser une liste pour stocker les incohérences\n",
				"inconsistencies = []\n",
				"\n",
				"# Vérification des incohérences dans les données\n",
				"# Parcourir chaque pays et vérifier les incohérences\n",
				"for country, group in df_grouped.groupby('Country'):\n",
				"    group = group.sort_values('Month').reset_index(drop=True)\n",
				"\n",
				"\n",
				"    for i in range(1, len(group)):\n",
				"        current = group.iloc[i]\n",
				"        prev = group.iloc[i - 1]\n",
				"\n",
				"        # Vérification décès\n",
				"        expected_deaths = prev['Deaths'] + current['New deaths']\n",
				"        if abs(current['Deaths'] - expected_deaths) > MARGIN:\n",
				"            print(f\"⚠️ Incohérence pour '{country}' au mois de {current['Month']} [Deaths]: \"\n",
				"                  f\"{current['Deaths']} ≠ {prev['Deaths']} + {current['New deaths']} (tolérance ±{MARGIN})\")\n",
				"            inconsistencies.append({\n",
				"                'Country': country,\n",
				"                'Month': current['Month'],\n",
				"                'Field': 'Deaths',\n",
				"                'Actual': current['Deaths'],\n",
				"                'Expected': expected_deaths\n",
				"            })\n",
				"\n",
				"        # Vérification cas confirmés\n",
				"        expected_confirmed = prev['Confirmed'] + current['New cases']\n",
				"        if abs(current['Confirmed'] - expected_confirmed) > MARGIN:\n",
				"            print(f\"⚠️ Incohérence pour '{country}' au mois de {current['Month']} [Confirmed]: \"\n",
				"                  f\"{current['Confirmed']} ≠ {prev['Confirmed']} + {current['New cases']} (tolérance ±{MARGIN})\")\n",
				"            inconsistencies.append({\n",
				"                'Country': country,\n",
				"                'Month': current['Month'],\n",
				"                'Field': 'Confirmed',\n",
				"                'Actual': current['Confirmed'],\n",
				"                'Expected': expected_confirmed\n",
				"            })\n",
				"\n",
				"        # Vérification récupérations\n",
				"        expected_recovered = prev['Recovered'] + current['New recovered']\n",
				"        if abs(current['Recovered'] - expected_recovered) > MARGIN:\n",
				"            print(f\"⚠️ Incohérence pour '{country}' au mois de {current['Month']} [Recovered]: \"\n",
				"                  f\"{current['Recovered']} ≠ {prev['Recovered']} + {current['New recovered']} (tolérance ±{MARGIN})\")\n",
				"            inconsistencies.append({\n",
				"                'Country': country,\n",
				"                'Month': current['Month'],\n",
				"                'Field': 'Recovered',\n",
				"                'Actual': current['Recovered'],\n",
				"                'Expected': expected_recovered\n",
				"            })\n",
				"\n",
				"\n",
				"inconsistencies_df = pd.DataFrame(inconsistencies)\n",
				"inconsistencies_df.head()\n",
				"\n",
				"# Sauvegarder le résultat dans le répertoire courant\n",
				"df_grouped.to_csv(\"data_etl_output.csv\", index=False)\n",
				"\n",
				"print(\"\\nETL terminé. Fichier sauvegardé sous : data_etl_output.csv\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 2,
			"id": "9548065c",
			"metadata": {},
			"outputs": [],
			"source": [
				"import pandas as pd\n",
				"from sklearn.model_selection import train_test_split\n",
				"from sklearn.preprocessing import LabelEncoder\n",
				"\n",
				"# Charger les données\n",
				"df = pd.read_csv(\"data_etl_output.csv\")\n",
				"\n",
				"# Créer des colonnes décalées pour les prédictions\n",
				"for col in ['Confirmed', 'Deaths', 'Recovered']:\n",
				"    df[f'{col}_lag1'] = df.groupby('Country')[col].shift(1)\n",
				"\n",
				"# Supprimer les lignes avec des valeurs manquantes dues au décalage\n",
				"df = df.dropna()\n",
				"\n",
				"# Encoder les pays\n",
				"le = LabelEncoder()\n",
				"df['Country_encoded'] = le.fit_transform(df['Country'])\n",
				"\n",
				"# Définir les features et la target\n",
				"features = ['Confirmed_lag1', 'Deaths_lag1', 'Recovered_lag1', 'Country_encoded']\n",
				"targets = ['Confirmed', 'Deaths', 'Recovered']\n",
				"\n",
				"X = df[features]\n",
				"y = df[targets]\n",
				"\n",
				"#load the dataset \n",
				"\n",
				"\n",
				"# Diviser les données en ensembles d'entraînement et de test\n",
				"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
				"\n",
				"\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 3,
			"id": "68f0ec33",
			"metadata": {
				"scrolled": true
			},
			"outputs": [
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"Random Forest Regressor RMSE:\n",
						"Confirmed: 56527.16753956689, Deaths: 3834.3655447739757, Recovered: 24347.824072305015\n",
						"Gradient Boosting Regressor RMSE:\n",
						"Confirmed: 80006.05882751447, Deaths: 2279.5740766256695, Recovered: 30815.436802704888\n",
						"Linear Regression RMSE:\n",
						"Confirmed: 55424.77044972432, Deaths: 2546.735710281176, Recovered: 41015.71417962556\n",
						"\n",
						"RMSE scores:\n",
						"Random Forest Regressor R2:\n",
						"Confirmed: 0.8119107871463432, Deaths: 0.6582321397024737, Recovered: 0.9115087087356599\n",
						"Gradient Boosting Regressor R2:\n",
						"Confirmed: 0.6232136209458147, Deaths: 0.8792042619829725, Recovered: 0.8582520200233096\n",
						"Linear Regression R2:\n",
						"Confirmed: 0.8191755110616417, Deaths: 0.8492310367278773, Recovered: 0.748880303194476\n"
					]
				}
			],
			"source": [
				"from sklearn.multioutput import MultiOutputRegressor\n",
				"from sklearn.ensemble import GradientBoostingRegressor\n",
				"from sklearn.ensemble import RandomForestRegressor\n",
				"from sklearn.linear_model import LinearRegression\n",
				"from sklearn.metrics import mean_squared_error\n",
				"import numpy as np\n",
				"\n",
				"\n",
				"rfr_model = RandomForestRegressor(random_state=42)\n",
				"rfr_model.fit(X_train, y_train)\n",
				"\n",
				"rfr_rmse_confirmed = np.sqrt(mean_squared_error(y_test['Confirmed'], rfr_model.predict(X_test)[:, 0]))\n",
				"rfr_rmse_deaths = np.sqrt(mean_squared_error(y_test['Deaths'], rfr_model.predict(X_test)[:, 1]))\n",
				"rfr_rmse_recovered = np.sqrt(mean_squared_error(y_test['Recovered'], rfr_model.predict(X_test)[:, 2]))\n",
				"\n",
				"print(\"Random Forest Regressor RMSE:\")\n",
				"print(f\"Confirmed: {rfr_rmse_confirmed}, Deaths: {rfr_rmse_deaths}, Recovered: {rfr_rmse_recovered}\")\n",
				"\n",
				"# Gradient Boosting Regressor with MultiOutputRegressor\n",
				"gbr_model = MultiOutputRegressor(GradientBoostingRegressor(random_state=42))\n",
				"gbr_model.fit(X_train, y_train)\n",
				"\n",
				"# Predictions with Gradient Boosting\n",
				"gbr_y_pred = gbr_model.predict(X_test)\n",
				"\n",
				"# Calculate RMSE for Gradient Boosting\n",
				"gbr_rmse_confirmed = np.sqrt(mean_squared_error(y_test['Confirmed'], gbr_y_pred[:, 0]))\n",
				"gbr_rmse_deaths = np.sqrt(mean_squared_error(y_test['Deaths'], gbr_y_pred[:, 1]))\n",
				"gbr_rmse_recovered = np.sqrt(mean_squared_error(y_test['Recovered'], gbr_y_pred[:, 2]))\n",
				"\n",
				"print(\"Gradient Boosting Regressor RMSE:\")\n",
				"print(f\"Confirmed: {gbr_rmse_confirmed}, Deaths: {gbr_rmse_deaths}, Recovered: {gbr_rmse_recovered}\")\n",
				"\n",
				"# Linear Regression with MultiOutputRegressor\n",
				"lr_model = MultiOutputRegressor(LinearRegression())\n",
				"lr_model.fit(X_train, y_train)\n",
				"\n",
				"# Predictions with Linear Regression\n",
				"lr_y_pred = lr_model.predict(X_test)\n",
				"\n",
				"# Calculate RMSE for Linear Regression\n",
				"lr_rmse_confirmed = np.sqrt(mean_squared_error(y_test['Confirmed'], lr_y_pred[:, 0]))\n",
				"lr_rmse_deaths = np.sqrt(mean_squared_error(y_test['Deaths'], lr_y_pred[:, 1]))\n",
				"lr_rmse_recovered = np.sqrt(mean_squared_error(y_test['Recovered'], lr_y_pred[:, 2]))\n",
				"\n",
				"print(\"Linear Regression RMSE:\")\n",
				"print(f\"Confirmed: {lr_rmse_confirmed}, Deaths: {lr_rmse_deaths}, Recovered: {lr_rmse_recovered}\")\n",
				"\n",
				"print(\"\\nRMSE scores:\")\n",
				"# score r2 random forest\n",
				"from sklearn.metrics import r2_score\n",
				"rfr_r2_confirmed = r2_score(y_test['Confirmed'], rfr_model.predict(X_test)[:, 0])\n",
				"rfr_r2_deaths = r2_score(y_test['Deaths'], rfr_model.predict(X_test)[:, 1])\n",
				"rfr_r2_recovered = r2_score(y_test['Recovered'], rfr_model.predict(X_test)[:, 2])\n",
				"print(\"Random Forest Regressor R2:\")\n",
				"print(f\"Confirmed: {rfr_r2_confirmed}, Deaths: {rfr_r2_deaths}, Recovered: {rfr_r2_recovered}\")\n",
				"\n",
				"# score r2 gradient boosting\n",
				"gbr_r2_confirmed = r2_score(y_test['Confirmed'], gbr_y_pred[:, 0])\n",
				"gbr_r2_deaths = r2_score(y_test['Deaths'], gbr_y_pred[:, 1])\n",
				"gbr_r2_recovered = r2_score(y_test['Recovered'], gbr_y_pred[:, 2])\n",
				"print(\"Gradient Boosting Regressor R2:\")\n",
				"print(f\"Confirmed: {gbr_r2_confirmed}, Deaths: {gbr_r2_deaths}, Recovered: {gbr_r2_recovered}\")\n",
				"# score r2 linear regression\n",
				"lr_r2_confirmed = r2_score(y_test['Confirmed'], lr_y_pred[:, 0])\n",
				"lr_r2_deaths = r2_score(y_test['Deaths'], lr_y_pred[:, 1])\n",
				"lr_r2_recovered = r2_score(y_test['Recovered'], lr_y_pred[:, 2])\n",
				"print(\"Linear Regression R2:\")\n",
				"print(f\"Confirmed: {lr_r2_confirmed}, Deaths: {lr_r2_deaths}, Recovered: {lr_r2_recovered}\")\n"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 4,
			"id": "fd637ef7-df83-4ded-bb7a-49c826d34b15",
			"metadata": {},
			"outputs": [
				{
					"name": "stderr",
					"output_type": "stream",
					"text": [
						"2025/05/28 21:31:47 WARNING mlflow.utils.git_utils: Failed to import Git (the Git executable is probably not on your PATH), so Git SHA is not available. Error: Failed to initialize: Bad git executable.\n",
						"The git executable must be specified in one of the following ways:\n",
						"    - be included in your $PATH\n",
						"    - be set via $GIT_PYTHON_GIT_EXECUTABLE\n",
						"    - explicitly set via git.refresh(<full-path-to-git-executable>)\n",
						"\n",
						"All git commands will error until this is rectified.\n",
						"\n",
						"This initial message can be silenced or aggravated in the future by setting the\n",
						"$GIT_PYTHON_REFRESH environment variable. Use one of the following values:\n",
						"    - quiet|q|silence|s|silent|none|n|0: for no message or exception\n",
						"    - warn|w|warning|log|l|1: for a warning message (logging level CRITICAL, displayed by default)\n",
						"    - error|e|exception|raise|r|2: for a raised exception\n",
						"\n",
						"Example:\n",
						"    export GIT_PYTHON_REFRESH=quiet\n",
						"\n",
						"\u001b[31m2025/05/28 21:31:58 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
						"\u001b[31m2025/05/28 21:32:08 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n",
						"\u001b[31m2025/05/28 21:32:13 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n"
					]
				},
				{
					"name": "stdout",
					"output_type": "stream",
					"text": [
						"✅ MLflow logging completed!\n",
						"Pour visualiser les résultats\n"
					]
				}
			],
			"source": [
				"import mlflow\n",
				"import mlflow.sklearn\n",
				"import matplotlib.pyplot as plt\n",
				"import seaborn as sns\n",
				"import numpy as np\n",
				"from sklearn.model_selection import learning_curve\n",
				"\n",
				"# 1. Initialisation de l'expérience\n",
				"experiment_name = \"COVID-19_Predictions\"\n",
				"if not mlflow.get_experiment_by_name(experiment_name):\n",
				"    mlflow.create_experiment(experiment_name)\n",
				"mlflow.set_experiment(experiment_name)\n",
				"\n",
				"# 2. Fonction pour générer les artefacts\n",
				"def generate_artifacts(model, model_name, X_train, y_train, rmse_values, r2_values):\n",
				"    \"\"\"Génère et enregistre tous les artefacts pour un modèle\"\"\"\n",
				"    # Heatmap RMSE\n",
				"    plt.figure(figsize=(8, 3))\n",
				"    sns.heatmap([rmse_values], annot=True, fmt=\".2f\", \n",
				"                xticklabels=['Confirmed', 'Deaths', 'Recovered'],\n",
				"                cmap=\"YlOrRd\")\n",
				"    plt.title(f\"RMSE - {model_name}\")\n",
				"    mlflow.log_figure(plt.gcf(), f\"rmse_heatmap.png\")\n",
				"    plt.close()\n",
				"    \n",
				"    # Heatmap R2\n",
				"    plt.figure(figsize=(8, 3))\n",
				"    sns.heatmap([r2_values], annot=True, fmt=\".2f\",\n",
				"                xticklabels=['Confirmed', 'Deaths', 'Recovered'],\n",
				"                cmap=\"YlGnBu\", vmin=-1, vmax=1)\n",
				"    plt.title(f\"R² - {model_name}\")\n",
				"    mlflow.log_figure(plt.gcf(), f\"r2_heatmap.png\")\n",
				"    plt.close()\n",
				"    \n",
				"    # Learning Curve\n",
				"    train_sizes, train_scores, val_scores = learning_curve(\n",
				"        model, X_train, y_train, cv=3, scoring='r2')\n",
				"    plt.figure(figsize=(8, 4))\n",
				"    plt.plot(train_sizes, np.mean(train_scores, axis=1), label=\"Train\")\n",
				"    plt.plot(train_sizes, np.mean(val_scores, axis=1), label=\"Validation\")\n",
				"    plt.title(f\"Learning Curve - {model_name}\")\n",
				"    mlflow.log_figure(plt.gcf(), f\"learning_curve.png\")\n",
				"    plt.close()\n",
				"    \n",
				"    # Feature Importance (si applicable)\n",
				"    if hasattr(model, 'feature_importances_'):\n",
				"        plt.figure(figsize=(10, 3))\n",
				"        sns.barplot(x=X_train.columns, y=model.feature_importances_)\n",
				"        plt.title(f\"Feature Importance - {model_name}\")\n",
				"        mlflow.log_figure(plt.gcf(), f\"feature_importance.png\")\n",
				"        plt.close()\n",
				"    elif model_name == \"GradientBoosting\":  # Cas spécifique pour MultiOutputRegressor\n",
				"        try:\n",
				"            plt.figure(figsize=(10, 3))\n",
				"            sns.barplot(x=X_train.columns, y=model.estimators_[0].feature_importances_)\n",
				"            plt.title(f\"Feature Importance - {model_name} (First Target)\")\n",
				"            mlflow.log_figure(plt.gcf(), f\"feature_importance.png\")\n",
				"            plt.close()\n",
				"        except Exception as e:\n",
				"            print(f\"Could not generate feature importance for {model_name}: {str(e)}\")\n",
				"\n",
				"# 3. Fonction pour créer les heatmaps globales\n",
				"def create_global_heatmaps():\n",
				"    \"\"\"Crée et enregistre les heatmaps comparatives de tous les modèles\"\"\"\n",
				"    models = ['RandomForest', 'GradientBoosting', 'LinearRegression']\n",
				"    \n",
				"    # RMSE global\n",
				"    rmse_data = np.array([\n",
				"        [rfr_rmse_confirmed, rfr_rmse_deaths, rfr_rmse_recovered],\n",
				"        [gbr_rmse_confirmed, gbr_rmse_deaths, gbr_rmse_recovered],\n",
				"        [lr_rmse_confirmed, lr_rmse_deaths, lr_rmse_recovered]\n",
				"    ])\n",
				"    \n",
				"    plt.figure(figsize=(10, 5))\n",
				"    sns.heatmap(rmse_data, annot=True, fmt=\".2f\",\n",
				"                xticklabels=['Confirmed', 'Deaths', 'Recovered'],\n",
				"                yticklabels=models,\n",
				"                cmap=\"YlOrRd\")\n",
				"    plt.title(\"Comparaison globale des RMSE\")\n",
				"    mlflow.log_figure(plt.gcf(), \"global_rmse_comparison.png\")\n",
				"    plt.close()\n",
				"    \n",
				"    # R2 global\n",
				"    r2_data = np.array([\n",
				"        [rfr_r2_confirmed, rfr_r2_deaths, rfr_r2_recovered],\n",
				"        [gbr_r2_confirmed, gbr_r2_deaths, gbr_r2_recovered],\n",
				"        [lr_r2_confirmed, lr_r2_deaths, lr_r2_recovered]\n",
				"    ])\n",
				"    \n",
				"    plt.figure(figsize=(10, 5))\n",
				"    sns.heatmap(r2_data, annot=True, fmt=\".2f\",\n",
				"                xticklabels=['Confirmed', 'Deaths', 'Recovered'],\n",
				"                yticklabels=models,\n",
				"                cmap=\"YlGnBu\", vmin=-1, vmax=1)\n",
				"    plt.title(\"Comparaison globale des R²\")\n",
				"    mlflow.log_figure(plt.gcf(), \"global_r2_comparison.png\")\n",
				"    plt.close()\n",
				"\n",
				"# 4. Fonction pour logger un modèle\n",
				"def log_model_run(model, model_name, metrics, X_train, y_train):\n",
				"    with mlflow.start_run(run_name=model_name):\n",
				"        mlflow.log_metrics(metrics)\n",
				"        generate_artifacts(model, model_name, X_train, y_train,\n",
				"                         [metrics['rmse_confirmed'], metrics['rmse_deaths'], metrics['rmse_recovered']],\n",
				"                         [metrics['r2_confirmed'], metrics['r2_deaths'], metrics['r2_recovered']])\n",
				"        mlflow.sklearn.log_model(model, \"model\")\n",
				"\n",
				"# 5. Exécution principale\n",
				"# D'abord les visualisations globales dans un run dédié\n",
				"with mlflow.start_run(run_name=\"Global_Visualizations\"):\n",
				"    create_global_heatmaps()\n",
				"\n",
				"# Puis les runs pour chaque modèle\n",
				"models_data = {\n",
				"    \"RandomForest\": {\n",
				"        \"model\": rfr_model,\n",
				"        \"metrics\": {\n",
				"            \"rmse_confirmed\": rfr_rmse_confirmed,\n",
				"            \"rmse_deaths\": rfr_rmse_deaths,\n",
				"            \"rmse_recovered\": rfr_rmse_recovered,\n",
				"            \"r2_confirmed\": rfr_r2_confirmed,\n",
				"            \"r2_deaths\": rfr_r2_deaths,\n",
				"            \"r2_recovered\": rfr_r2_recovered\n",
				"        }\n",
				"    },\n",
				"    \"GradientBoosting\": {\n",
				"        \"model\": gbr_model,\n",
				"        \"metrics\": {\n",
				"            \"rmse_confirmed\": gbr_rmse_confirmed,\n",
				"            \"rmse_deaths\": gbr_rmse_deaths,\n",
				"            \"rmse_recovered\": gbr_rmse_recovered,\n",
				"            \"r2_confirmed\": gbr_r2_confirmed,\n",
				"            \"r2_deaths\": gbr_r2_deaths,\n",
				"            \"r2_recovered\": gbr_r2_recovered\n",
				"        }\n",
				"    },\n",
				"    \"LinearRegression\": {\n",
				"        \"model\": lr_model,\n",
				"        \"metrics\": {\n",
				"            \"rmse_confirmed\": lr_rmse_confirmed,\n",
				"            \"rmse_deaths\": lr_rmse_deaths,\n",
				"            \"rmse_recovered\": lr_rmse_recovered,\n",
				"            \"r2_confirmed\": lr_r2_confirmed,\n",
				"            \"r2_deaths\": lr_r2_deaths,\n",
				"            \"r2_recovered\": lr_r2_recovered\n",
				"        }\n",
				"    }\n",
				"}\n",
				"\n",
				"for model_name, data in models_data.items():\n",
				"    log_model_run(data[\"model\"], model_name, data[\"metrics\"], X_train, y_train)\n",
				"\n",
				"print(\"✅ MLflow logging completed!\")\n",
				"print(f\"Pour visualiser les résultats\")"
			]
		},
		{
			"cell_type": "code",
			"execution_count": 5,
			"id": "a8a66723",
			"metadata": {},
			"outputs": [
				{
					"ename": "FileNotFoundError",
					"evalue": "[Errno 2] No such file or directory: '../api/model.pkl'",
					"output_type": "error",
					"traceback": [
						"\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
						"\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
						"Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Créer le pickle pour le modèle Random Forest\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../api/model.pkl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mwb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      4\u001b[0m     pickle\u001b[38;5;241m.\u001b[39mdump(rfr_model, f)\n",
						"File \u001b[0;32m/usr/local/lib/python3.9/site-packages/IPython/core/interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    308\u001b[0m     )\n\u001b[0;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
						"\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../api/model.pkl'"
					]
				}
			],
			"source": [
				"# Créer le pickle pour le modèle Random Forest\n",
				"import pickle\n",
				"with open('../api/model.pkl', 'wb') as f:\n",
				"    pickle.dump(rfr_model, f)"
			]
		}
	],
	"metadata": {
		"kernelspec": {
			"display_name": ".venv",
			"language": "python",
			"name": "python3"
		},
		"language_info": {
			"codemirror_mode": {
				"name": "ipython",
				"version": 3
			},
			"file_extension": ".py",
			"mimetype": "text/x-python",
			"name": "python",
			"nbconvert_exporter": "python",
			"pygments_lexer": "ipython3",
			"version": "3.9.22"
		}
	},
	"nbformat": 4,
	"nbformat_minor": 5
}
